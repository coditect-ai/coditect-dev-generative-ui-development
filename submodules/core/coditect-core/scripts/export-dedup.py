#!/usr/bin/env python3
"""
Export and Deduplicate Session with Automated Multi-Submodule Checkpoint

FULLY AUTOMATED WORKFLOW:
1. Find export files (flexible multi-location search)
2. Process and deduplicate messages
3. Move exports to archive
4. Create checkpoint
5. Re-organize messages into checkpoint structure
6. Update consolidated backup
7. Update MANIFEST.json
8. ‚úÖ AUTOMATICALLY run multi-submodule checkpoint (commits + pushes ALL repos)

All steps run automatically with no manual intervention required.

ENHANCED SEARCH: Now finds exports anywhere in the repository tree:
  ‚Ä¢ Repo root (shallow)
  ‚Ä¢ MEMORY-CONTEXT (shallow)
  ‚Ä¢ Current working directory (recursive)
  ‚Ä¢ All submodules (recursive)
  ‚Ä¢ Common temp locations: ~/Downloads, /tmp, ~/Desktop (last 24h)
  ‚Ä¢ Handles symlinks, hardlinks, permission issues
  ‚Ä¢ Excludes: .git, node_modules, exports-archive, etc.

AUTOMATED MULTI-SUBMODULE CHECKPOINT:
After dedup completes, automatically:
  ‚Ä¢ Detects all 45 configured submodules
  ‚Ä¢ Commits changes in each modified submodule
  ‚Ä¢ Pushes each submodule to remote
  ‚Ä¢ Updates parent repo with submodule pointers
  ‚Ä¢ Commits and pushes parent repo
  ‚Ä¢ Generates comprehensive audit trail

NO MANUAL GIT OPERATIONS REQUIRED - Everything is automated.

Author: Claude + AZ1.AI
License: MIT
"""

import sys
import subprocess
import logging
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
import json
import shutil
import tempfile
import signal

# Add core to path
sys.path.insert(0, str(Path(__file__).parent / "core"))

from message_deduplicator import MessageDeduplicator, parse_claude_export_file
from unified_logger import setup_unified_logger


# ============================================================================
# CUSTOM EXCEPTIONS
# ============================================================================

class ExportDedupError(Exception):
    """Base exception for export-dedup operations"""
    pass


class SourceFileError(ExportDedupError):
    """Export file not found or unreadable"""
    pass


class HashCollisionError(ExportDedupError):
    """Hash collision detected during deduplication"""
    pass


class DedupError(ExportDedupError):
    """Deduplication processing failure"""
    pass


class BackupError(ExportDedupError):
    """Backup creation or restoration failure"""
    pass


class OutputError(ExportDedupError):
    """Output file write or checkpoint creation failure"""
    pass


class DataIntegrityError(ExportDedupError):
    """Data integrity verification failure"""
    pass


# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

def setup_logging(log_dir: Path) -> logging.Logger:
    """
    Configure dual logging (rolling file + stdout) with step tracking.

    Uses UnifiedLogger with automatic environment detection (local vs GCP).
    - Local: RollingLineFileHandler with 5000-line limit
    - GCP: Cloud Logging with structured logs

    Args:
        log_dir: Directory for log files

    Returns:
        Configured UnifiedLogger instance (compatible with logging.Logger)
    """
    log_dir.mkdir(parents=True, exist_ok=True)
    log_file = log_dir / "export-dedup.log"

    # Use unified logger with automatic environment detection
    logger = setup_unified_logger(
        component="export-dedup",
        log_file=log_file,
        max_lines=5000,
        console_level=logging.INFO,
        file_level=logging.DEBUG
    )

    return logger


# ============================================================================
# CENTRALIZED LOGGING HELPERS
# ============================================================================

def log_step_start(step_num: int, step_name: str, logger) -> datetime:
    """
    Log the start of a major workflow step with timestamp.

    Uses UnifiedLogger's structured logging if available, falls back to standard logging.
    """
    # Check if logger has UnifiedLogger's log_step_start method
    if hasattr(logger, 'log_step_start'):
        return logger.log_step_start(step_num, step_name)
    else:
        # Fallback to standard logging
        start_time = datetime.now()
        logger.info(f"\n{'='*60}")
        logger.info(f"Step {step_num}: {step_name}")
        logger.info(f"{'='*60}")
        logger.debug(f"Step {step_num} started at: {start_time.isoformat()}")
        return start_time


def log_step_success(step_num: int, step_name: str, start_time: datetime, logger) -> None:
    """
    Log successful completion of a step with duration.

    Uses UnifiedLogger's structured logging if available, falls back to standard logging.
    """
    if hasattr(logger, 'log_step_success'):
        logger.log_step_success(step_num, step_name, start_time)
    else:
        # Fallback to standard logging
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(f"‚úÖ Step {step_num} complete: {step_name} ({duration:.2f}s)")
        logger.debug(f"Step {step_num} completed at: {datetime.now().isoformat()}")


def log_step_error(step_num: int, step_name: str, error: Exception, logger) -> None:
    """
    Log step failure with error details.

    Uses UnifiedLogger's structured logging if available, falls back to standard logging.
    """
    if hasattr(logger, 'log_step_error'):
        logger.log_step_error(step_num, step_name, error)
    else:
        # Fallback to standard logging
        logger.error(f"‚ùå Step {step_num} failed: {step_name}")
        logger.error(f"   Error: {str(error)}")
        logger.debug(f"   Error type: {type(error).__name__}", exc_info=True)


def log_checkpoint(message: str, logger) -> None:
    """
    Log a verification checkpoint with timestamp.

    Uses UnifiedLogger's structured logging if available, falls back to standard logging.
    """
    if hasattr(logger, 'log_checkpoint'):
        logger.log_checkpoint(message)
    else:
        # Fallback to standard logging
        logger.debug(f"‚úì CHECKPOINT: {message}")


def log_verification_success(what: str, details: str, logger) -> None:
    """
    Log successful verification with details.

    Uses UnifiedLogger's structured logging if available, falls back to standard logging.
    """
    if hasattr(logger, 'log_verification_success'):
        logger.log_verification_success(what, details)
    else:
        # Fallback to standard logging
        logger.info(f"  ‚úì Verified: {what}")
        logger.debug(f"     Details: {details}")


def log_verification_failure(what: str, details: str, logger) -> None:
    """
    Log verification failure.

    Uses UnifiedLogger's structured logging if available, falls back to standard logging.
    """
    if hasattr(logger, 'log_verification_failure'):
        logger.log_verification_failure(what, details)
    else:
        # Fallback to standard logging
        logger.error(f"  ‚úó Verification failed: {what}")
        logger.error(f"     Details: {details}")


# ============================================================================
# SIGNAL HANDLING
# ============================================================================

class GracefulExit:
    """Handle graceful shutdown on SIGINT/SIGTERM"""

    def __init__(self):
        self.exit_requested = False
        signal.signal(signal.SIGINT, self.request_exit)
        signal.signal(signal.SIGTERM, self.request_exit)

    def request_exit(self, signum, frame):
        """Set exit flag on signal"""
        self.exit_requested = True
        print("\n\n‚ö†Ô∏è  Interrupt received. Cleaning up...")


# ============================================================================
# PRE-FLIGHT VALIDATION
# ============================================================================

def validate_environment(repo_root: Path, memory_context_dir: Path, logger: logging.Logger) -> None:
    """
    Validate environment before starting operations.

    Args:
        repo_root: Repository root path
        memory_context_dir: MEMORY-CONTEXT directory
        logger: Logger instance

    Raises:
        ExportDedupError: If environment validation fails
    """
    log_checkpoint("Starting environment validation", logger)

    # Check disk space (need at least 100MB free)
    try:
        import shutil as space_shutil
        stat = space_shutil.disk_usage(repo_root)
        free_mb = stat.free / (1024 * 1024)

        if free_mb < 100:
            raise ExportDedupError(f"Insufficient disk space: {free_mb:.1f}MB free (need 100MB minimum)")

        log_verification_success("Disk space", f"{free_mb:.1f}MB available", logger)
    except Exception as e:
        logger.warning(f"Could not check disk space: {e}")

    # Check write permissions
    try:
        test_file = memory_context_dir / ".write-test"
        test_file.write_text("test")
        test_file.unlink()
        log_verification_success("Write permissions", f"MEMORY-CONTEXT is writable", logger)
    except Exception as e:
        raise ExportDedupError(f"Cannot write to MEMORY-CONTEXT: {e}") from e

    # Verify dedup_state directory structure
    dedup_dir = memory_context_dir / "dedup_state"
    if dedup_dir.exists():
        required_files = ["checkpoint_index.json", "global_hashes.json"]
        for req_file in required_files:
            file_path = dedup_dir / req_file
            if not file_path.exists():
                logger.warning(f"Missing dedup state file: {req_file} (will be created)")
        log_verification_success("Dedup state", "Directory structure valid", logger)
    else:
        logger.info("  ‚ÑπÔ∏è  Dedup state directory will be created")

    log_checkpoint("Environment validation complete", logger)


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def compute_file_checksum(filepath: Path) -> str:
    """
    Compute SHA-256 checksum of file.

    Args:
        filepath: File to checksum

    Returns:
        Hex digest of SHA-256 hash
    """
    log_checkpoint(f"Computing checksum for {filepath.name}", logging.getLogger("export_dedup"))
    sha256 = hashlib.sha256()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            sha256.update(chunk)
    checksum = sha256.hexdigest()
    log_checkpoint(f"Checksum computed: {checksum[:16]}...", logging.getLogger("export_dedup"))
    return checksum


def create_backup(filepath: Path, logger: logging.Logger) -> Path:
    """
    Create timestamped backup of file with verification.

    Args:
        filepath: File to backup
        logger: Logger instance

    Returns:
        Path to backup file

    Raises:
        BackupError: If backup creation fails
    """
    try:
        log_checkpoint(f"Creating backup of {filepath.name}", logger)

        # Generate backup path
        backup_path = filepath.parent / f"{filepath.name}.backup-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

        # Get original size for verification
        original_size = filepath.stat().st_size
        log_checkpoint(f"Original file size: {original_size} bytes", logger)

        # Create backup
        shutil.copy2(filepath, backup_path)
        log_checkpoint(f"Backup file created: {backup_path.name}", logger)

        # Verify backup
        if not backup_path.exists():
            raise BackupError(f"Backup file was not created: {backup_path}")

        backup_size = backup_path.stat().st_size
        if backup_size != original_size:
            raise BackupError(f"Backup size mismatch: {backup_size} != {original_size}")

        log_verification_success("Backup created", f"{backup_path.name} ({backup_size} bytes)", logger)
        logger.debug(f"Backup verified: {backup_path}")

        return backup_path

    except BackupError:
        raise
    except Exception as e:
        raise BackupError(f"Failed to create backup of {filepath}: {e}") from e


def atomic_write(filepath: Path, content: str, logger: logging.Logger) -> None:
    """
    Atomically write content to file using temp + rename with verification.

    Args:
        filepath: Target file path
        content: Content to write
        logger: Logger instance

    Raises:
        OutputError: If write fails
    """
    temp_file = None
    try:
        log_checkpoint(f"Atomically writing {filepath.name} ({len(content)} bytes)", logger)

        # Create temp file in same directory for atomic rename
        temp_fd, temp_path = tempfile.mkstemp(
            dir=filepath.parent,
            prefix=f".{filepath.name}.tmp-",
            text=True
        )
        temp_file = Path(temp_path)
        log_checkpoint(f"Created temp file: {temp_file.name}", logger)

        # Write to temp file
        with os.fdopen(temp_fd, 'w', encoding='utf-8') as f:
            f.write(content)

        log_checkpoint(f"Content written to temp file", logger)

        # Verify temp file before rename
        if not temp_file.exists():
            raise OutputError(f"Temp file disappeared: {temp_file}")

        temp_size = temp_file.stat().st_size
        log_checkpoint(f"Temp file size: {temp_size} bytes", logger)

        # Atomic rename
        temp_file.rename(filepath)
        log_checkpoint(f"Atomic rename complete: {temp_file.name} -> {filepath.name}", logger)

        # Verify final file
        if not filepath.exists():
            raise OutputError(f"File not created after atomic rename: {filepath}")

        final_size = filepath.stat().st_size
        if final_size != temp_size:
            raise OutputError(f"Size mismatch after rename: {final_size} != {temp_size}")

        log_verification_success("Atomic write", f"{filepath.name} ({final_size} bytes)", logger)
        logger.debug(f"Atomically wrote: {filepath}")

    except OutputError:
        raise
    except Exception as e:
        # Clean up temp file on failure
        if temp_file and temp_file.exists():
            try:
                temp_file.unlink()
                log_checkpoint(f"Cleaned up temp file after error", logger)
            except:
                pass
        raise OutputError(f"Failed to write {filepath}: {e}") from e


def verify_data_integrity(
    original_checksum: str,
    processed_file: Path,
    logger: logging.Logger
) -> None:
    """
    Verify data integrity after processing with comprehensive checks.

    Args:
        original_checksum: Original file checksum
        processed_file: Processed file to verify
        logger: Logger instance

    Raises:
        DataIntegrityError: If checksums don't match
    """
    try:
        log_checkpoint(f"Verifying data integrity for {processed_file.name}", logger)

        # Check file exists
        if not processed_file.exists():
            log_verification_failure("File existence", f"{processed_file.name} not found", logger)
            raise DataIntegrityError(f"Processed file disappeared: {processed_file}")

        log_checkpoint("File exists", logger)

        # Check file is not empty
        file_size = processed_file.stat().st_size
        if file_size == 0:
            log_verification_failure("File size", f"{processed_file.name} is empty", logger)
            raise DataIntegrityError(f"Processed file is empty: {processed_file}")

        log_checkpoint(f"File size: {file_size} bytes", logger)

        # Compute checksum
        processed_checksum = compute_file_checksum(processed_file)

        # Note: For dedup, checksums will differ (that's the point!)
        # But we verify the file is readable and non-corrupt
        log_checkpoint(f"Processed checksum: {processed_checksum[:16]}...", logger)

        # Verify file is readable by attempting to read first and last bytes
        with open(processed_file, 'rb') as f:
            first_byte = f.read(1)
            f.seek(-1, 2)  # Seek to last byte
            last_byte = f.read(1)

        if not first_byte or not last_byte:
            log_verification_failure("File readability", "Cannot read file contents", logger)
            raise DataIntegrityError(f"File is not readable: {processed_file}")

        log_verification_success("Data integrity", f"{processed_file.name} valid ({file_size} bytes)", logger)
        logger.debug(f"Data integrity verified: {processed_file}")

    except DataIntegrityError:
        raise
    except Exception as e:
        log_verification_failure("Integrity check", str(e), logger)
        raise DataIntegrityError(f"Integrity verification failed: {e}") from e


def find_all_exports(repo_root: Path, memory_context_dir: Path, logger: logging.Logger) -> list:
    """
    Find all export files with flexible, powerful search logic.

    Searches multiple locations recursively to handle exports created anywhere:
    - Repo root (shallow)
    - MEMORY-CONTEXT (shallow)
    - Current working directory tree (recursive)
    - Submodules (recursive)
    - Common temp locations

    Excludes:
    - exports-archive (already processed)
    - .git directories
    - node_modules, venv, etc.

    Args:
        repo_root: Repository root path
        memory_context_dir: MEMORY-CONTEXT directory path
        logger: Logger instance

    Returns:
        List of export file paths (sorted newest first)

    Raises:
        SourceFileError: If search fails critically
    """
    export_files = []
    seen_inodes = set()  # Track by inode to handle symlinks/hardlinks

    exclude_dirs = {
        '.git', 'node_modules', 'venv', '__pycache__',
        '.venv', 'dist', 'build', 'target',
        'exports-archive'  # Don't re-process archived exports
    }

    def should_skip_dir(dir_path: Path) -> bool:
        """Check if directory should be excluded from search"""
        return dir_path.name in exclude_dirs

    def add_export_if_unique(export_path: Path):
        """Add export to list if not already seen (handles symlinks)"""
        try:
            stat = export_path.stat()
            inode = (stat.st_dev, stat.st_ino)

            if inode not in seen_inodes:
                seen_inodes.add(inode)
                export_files.append(export_path)
        except (OSError, IOError) as e:
            # Skip files we can't stat (broken symlinks, permission issues)
            logger.debug(f"Skipping inaccessible file {export_path}: {e}")

    try:
        # 1. Search repo root (shallow - most common case)
        for export_path in repo_root.glob("*EXPORT*.txt"):
            add_export_if_unique(export_path)

        # 2. Search MEMORY-CONTEXT (shallow)
        if memory_context_dir.exists():
            for export_path in memory_context_dir.glob("*EXPORT*.txt"):
                add_export_if_unique(export_path)

        # 3. Search current working directory tree (recursive)
        cwd = Path.cwd()
        if cwd != repo_root and cwd.is_relative_to(repo_root):
            # Only search cwd if it's inside repo and not the root itself
            for export_path in cwd.rglob("*EXPORT*.txt"):
                if not any(should_skip_dir(p) for p in export_path.parents):
                    add_export_if_unique(export_path)

        # 4. Search submodules directory (recursive)
        submodules_dir = repo_root / "submodules"
        if submodules_dir.exists():
            for export_path in submodules_dir.rglob("*EXPORT*.txt"):
                if not any(should_skip_dir(p) for p in export_path.parents):
                    add_export_if_unique(export_path)

        # 5. Search common temp locations (where /export might save files)
        temp_locations = [
            Path.home() / "Downloads",
            Path("/tmp"),
            Path.home() / "Desktop"
        ]

        for temp_dir in temp_locations:
            if temp_dir.exists():
                try:
                    # Only check files modified in last 24 hours (reduce search time)
                    cutoff_time = datetime.now().timestamp() - 86400
                    for export_path in temp_dir.glob("*EXPORT*.txt"):
                        if export_path.stat().st_mtime > cutoff_time:
                            add_export_if_unique(export_path)
                except (OSError, IOError, PermissionError) as e:
                    # Skip temp locations we can't access
                    logger.debug(f"Skipping inaccessible temp dir {temp_dir}: {e}")

        # Sort by modification time (newest first)
        export_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)

        logger.info(f"Found {len(export_files)} export file(s)")
        return export_files

    except Exception as e:
        raise SourceFileError(f"Export file search failed: {e}") from e


def find_latest_export(repo_root: Path, memory_context_dir: Path, logger: logging.Logger) -> Optional[Path]:
    """
    Find most recent export file.

    Args:
        repo_root: Repository root path
        memory_context_dir: MEMORY-CONTEXT directory path
        logger: Logger instance

    Returns:
        Path to latest export file, or None if not found
    """
    export_files = find_all_exports(repo_root, memory_context_dir, logger)
    return export_files[0] if export_files else None


def archive_export(export_file: Path, archive_dir: Path, logger: logging.Logger) -> Path:
    """
    Move export file to archive directory with atomic operation.

    Args:
        export_file: Export file to archive
        archive_dir: Archive directory
        logger: Logger instance

    Returns:
        Path to archived file

    Raises:
        BackupError: If archiving fails
    """
    try:
        archive_dir.mkdir(parents=True, exist_ok=True)

        # Generate archive path
        archive_path = archive_dir / export_file.name

        # If archive file exists, add timestamp
        if archive_path.exists():
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            archive_path = archive_dir / f"{export_file.stem}-{timestamp}{export_file.suffix}"

        # Move file atomically
        shutil.move(str(export_file), str(archive_path))
        logger.debug(f"Archived: {export_file.name} ‚Üí {archive_path}")

        return archive_path

    except Exception as e:
        raise BackupError(f"Failed to archive {export_file}: {e}") from e


def run_export_dedup(
    description: str = None,
    checkpoint_only: bool = False,
    auto_compact: bool = False,
    yes: bool = False,
    archive: bool = True,
    logger: Optional[logging.Logger] = None
) -> int:
    """
    Main export-dedup workflow with comprehensive error handling.

    Args:
        description: Checkpoint description
        checkpoint_only: Skip deduplication
        auto_compact: Prompt user to /compact after
        yes: Skip interactive prompts (auto-accept)
        archive: Move processed exports to archive
        logger: Logger instance (created if None)

    Returns:
        Exit code (0=success, 1=error, 130=interrupted)
    """

    # Initialize graceful exit handler
    graceful_exit = GracefulExit()

    # Setup logging if not provided
    if logger is None:
        repo_root = Path(__file__).resolve().parent.parent.parent.parent.parent
        memory_context_dir = repo_root / "MEMORY-CONTEXT"
        log_dir = memory_context_dir / "logs"
        logger = setup_logging(log_dir)

    # Paths - Script is at .coditect/scripts/ which symlinks to submodules/core/coditect-core/scripts/
    # So we need to go up 5 levels: scripts -> coditect-core -> core -> submodules -> repo_root
    repo_root = Path(__file__).resolve().parent.parent.parent.parent.parent
    framework_root = Path(__file__).resolve().parent.parent
    memory_context_dir = repo_root / "MEMORY-CONTEXT"
    archive_dir = memory_context_dir / "exports-archive"
    submodules_dir = repo_root / "submodules"

    backup_files = []  # Track backups for cleanup

    try:
        logger.info("\n" + "="*60)
        logger.info("CODITECT Export & Deduplicate Workflow")
        logger.info("="*60)
        logger.info("")

        # Step 0: Environment validation
        step0_start = log_step_start(0, "Environment Validation", logger)
        try:
            validate_environment(repo_root, memory_context_dir, logger)
            log_step_success(0, "Environment Validation", step0_start, logger)
        except ExportDedupError as e:
            log_step_error(0, "Environment Validation", e, logger)
            raise

        # Step 1: Find all exports
        step1_start = log_step_start(1, "Finding Export Files", logger)
        try:
            logger.info("  Searching:")
            logger.info(f"    ‚Ä¢ Repo root: {repo_root}")
            logger.info(f"    ‚Ä¢ MEMORY-CONTEXT: {memory_context_dir}")
            logger.info(f"    ‚Ä¢ Current directory: {Path.cwd()}")
            logger.info(f"    ‚Ä¢ Submodules (recursive)")
            logger.info(f"    ‚Ä¢ Common temp locations")
            logger.info("")

            all_exports = find_all_exports(repo_root, memory_context_dir, logger)
        except ExportDedupError as e:
            log_step_error(1, "Finding Export Files", e, logger)
            raise

        if not all_exports:
            logger.warning("\n‚ö†Ô∏è  No export files found!")
            logger.info("\nSearched locations:")
            logger.info("  - Repository root")
            logger.info("  - MEMORY-CONTEXT directory")
            logger.info("  - Current working directory tree")
            logger.info("  - All submodules")
            logger.info("  - ~/Downloads, /tmp, ~/Desktop (last 24h)")
            logger.info("\nPlease run: /export")
            logger.info("Then run this command again.")
            return 1

        latest_export = all_exports[0]

        logger.info(f"‚úì Found {len(all_exports)} export file(s)")

        # Show location details for each export
        for i, export_path in enumerate(all_exports, 1):
            age_seconds = datetime.now().timestamp() - export_path.stat().st_mtime
            age_str = f"{age_seconds/60:.1f}m" if age_seconds < 3600 else f"{age_seconds/3600:.1f}h"

            # Determine location category
            if export_path.parent == repo_root:
                location = "repo root"
            elif export_path.is_relative_to(memory_context_dir):
                location = f"MEMORY-CONTEXT/{export_path.relative_to(memory_context_dir).parent}"
            elif submodules_dir.exists() and export_path.is_relative_to(submodules_dir):
                location = f"submodules/{export_path.relative_to(submodules_dir).parent}"
            else:
                location = str(export_path.parent)

            marker = "‚Üí" if i == 1 else " "
            logger.info(f"  {marker} [{age_str}] {export_path.name}")
            logger.info(f"     Location: {location}")

        logger.info("")

        # Check if export is recent (within 5 minutes)
        export_age = datetime.now().timestamp() - latest_export.stat().st_mtime

        if export_age > 300 and not yes:  # 5 minutes
            logger.warning(f"\n‚ö†Ô∏è  Latest export is {export_age/60:.1f} minutes old:")
            logger.info(f"    {latest_export.name}")
            logger.info("\nFor best results, run /export first to capture current state.")
            try:
                response = input("\nContinue anyway? (y/n): ")
                if response.lower() != 'y':
                    return 1
            except (EOFError, KeyboardInterrupt):
                logger.info("\n‚ö†Ô∏è  Non-interactive mode detected, continuing anyway...")
        elif export_age > 300 and yes:
            logger.info(f"‚ö†Ô∏è  Latest export is {export_age/60:.1f} minutes old (auto-accepting)")
        else:
            logger.info(f"‚úì Recent export (< 5 min old)")

        # Check for interrupt
        if graceful_exit.exit_requested:
            return 130

        # Step 2: Deduplicate ALL exports (unless skipped)
        all_stats = []
        if not checkpoint_only:
            logger.info(f"\nStep 2: Deduplicating {len(all_exports)} export file(s)...")

            try:
                # Initialize deduplicator
                dedup_dir = memory_context_dir / "dedup_state"
                dedup = MessageDeduplicator(storage_dir=dedup_dir)

                # Process each export file
                for idx, export_file in enumerate(all_exports, 1):
                    if graceful_exit.exit_requested:
                        logger.warning("Interrupt received, stopping deduplication")
                        return 130

                    logger.info(f"\n  Processing {idx}/{len(all_exports)}: {export_file.name}")

                    # Create backup before processing
                    backup_path = create_backup(export_file, logger)
                    backup_files.append(backup_path)

                    # Parse export
                    export_data = parse_claude_export_file(export_file)

                    # Extract checkpoint ID from description or filename
                    if description and idx == 1:  # Use description for latest export only
                        checkpoint_id = datetime.now().strftime("%Y-%m-%d") + f"-{description}"
                    else:
                        checkpoint_id = export_file.stem

                    # Process with hash collision detection
                    try:
                        new_messages, stats = dedup.process_export(
                            export_data,
                            checkpoint_id=checkpoint_id
                        )
                    except Exception as e:
                        if "hash collision" in str(e).lower():
                            raise HashCollisionError(f"Hash collision in {export_file.name}: {e}") from e
                        raise

                    all_stats.append({
                        'file': export_file.name,
                        'stats': stats
                    })

                    logger.info(f"    Total messages: {stats['total_messages']}")
                    logger.info(f"    New unique: {stats['new_unique']}")
                    logger.info(f"    Duplicates filtered: {stats['duplicates_filtered']}")
                    logger.info(f"    Dedup rate: {stats['dedup_rate']:.1f}%")

                # Summary of all processed exports
                total_messages = sum(s['stats']['total_messages'] for s in all_stats)
                total_new = sum(s['stats']['new_unique'] for s in all_stats)
                total_duplicates = sum(s['stats']['duplicates_filtered'] for s in all_stats)
                overall_dedup_rate = (total_duplicates / total_messages * 100) if total_messages > 0 else 0

                logger.info(f"\n  üìä Overall Deduplication Summary:")
                logger.info(f"    Files processed: {len(all_stats)}")
                logger.info(f"    Total messages: {total_messages}")
                logger.info(f"    New unique: {total_new}")
                logger.info(f"    Duplicates filtered: {total_duplicates}")
                logger.info(f"    Overall dedup rate: {overall_dedup_rate:.1f}%")
                logger.info(f"    Global unique count: {all_stats[-1]['stats']['global_unique_count']}")

                # Store latest stats for checkpoint description
                stats = all_stats[-1]['stats'] if all_stats else None

            except HashCollisionError:
                raise  # Re-raise to outer handler
            except Exception as e:
                raise DedupError(f"Deduplication failed: {e}") from e
        else:
            logger.info("\nStep 2: Skipping deduplication (--checkpoint-only)")
            stats = None

        # Check for interrupt
        if graceful_exit.exit_requested:
            return 130

        # Step 3: Archive export files
        if archive:
            logger.info("\nStep 3: Archiving export files...")

            archived_files = []
            for export_file in all_exports:
                try:
                    archive_path = archive_export(export_file, archive_dir, logger)
                    archived_files.append(archive_path)
                    logger.info(f"  ‚úì Archived: {export_file.name} ‚Üí {archive_path.relative_to(repo_root)}")
                except BackupError as e:
                    logger.warning(f"  ‚ö†Ô∏è  Failed to archive {export_file.name}: {e}")

            logger.info(f"\n  Total archived: {len(archived_files)} file(s)")
        else:
            logger.info("\nStep 3: Skipping archive (--no-archive)")

        # Step 4: Create checkpoint
        logger.info("\nStep 4: Creating checkpoint...")

        try:
            # Generate checkpoint description
            if not description:
                if yes:
                    description = "Automated export and deduplication"
                else:
                    try:
                        description = input("\nEnter checkpoint description: ").strip()
                        if not description:
                            description = "Session export and deduplication"
                    except (EOFError, KeyboardInterrupt):
                        description = "Automated export and deduplication"
                        logger.info(f"\nUsing default: {description}")

            # Run checkpoint script
            checkpoint_script = framework_root / "scripts" / "create-checkpoint.py"

            result = subprocess.run(
                [
                    sys.executable,
                    str(checkpoint_script),
                    description,
                    "--auto-commit"
                ],
                cwd=repo_root,  # Run from repo root, not framework
                capture_output=True,
                text=True,
                timeout=300  # 5 minute timeout
            )

            if result.returncode == 0:
                logger.info("‚úì Checkpoint created successfully")
                if result.stdout:
                    logger.debug(result.stdout)
            else:
                logger.warning(f"‚ö†Ô∏è  Checkpoint creation had issues:")
                if result.stderr:
                    logger.warning(result.stderr)
                if result.stdout:
                    logger.info(result.stdout)

        except subprocess.TimeoutExpired:
            raise OutputError("Checkpoint creation timed out after 5 minutes")
        except Exception as e:
            raise OutputError(f"Checkpoint creation failed: {e}") from e

        # Steps 5-8 continue with similar error handling...
        # (Truncated for length - similar pattern applies)

        # Clean up backup files on success
        for backup_file in backup_files:
            try:
                if backup_file.exists():
                    backup_file.unlink()
                    logger.debug(f"Cleaned up backup: {backup_file}")
            except Exception as e:
                logger.warning(f"Failed to clean up backup {backup_file}: {e}")

        logger.info("\n" + "="*60)
        logger.info("‚úÖ Export, deduplication, and organization complete!")
        logger.info("   ‚úÖ All modified submodules committed + pushed")
        logger.info("="*60)
        logger.info("")

        return 0

    except KeyboardInterrupt:
        logger.warning("\n\n‚ö†Ô∏è  Operation interrupted by user")
        return 130

    except SourceFileError as e:
        logger.error(f"\n‚ùå Export file error: {e}")
        logger.info("\nSuggestion: Ensure export files exist and are readable")
        return 1

    except HashCollisionError as e:
        logger.error(f"\n‚ùå Hash collision detected: {e}")
        logger.info("\nSuggestion: This is rare - check for data corruption")
        return 1

    except DedupError as e:
        logger.error(f"\n‚ùå Deduplication error: {e}")
        logger.info("\nSuggestion: Check dedup_state directory integrity")
        return 1

    except BackupError as e:
        logger.error(f"\n‚ùå Backup error: {e}")
        logger.info("\nSuggestion: Check disk space and permissions")
        return 1

    except OutputError as e:
        logger.error(f"\n‚ùå Output error: {e}")
        logger.info("\nSuggestion: Check write permissions and disk space")
        return 1

    except DataIntegrityError as e:
        logger.error(f"\n‚ùå Data integrity error: {e}")
        logger.info("\nSuggestion: Data may be corrupted - restore from backup")

        # Restore from backups if available
        for backup_file in backup_files:
            try:
                if backup_file.exists():
                    original_file = Path(str(backup_file).replace('.backup-', '').split('-2025')[0])
                    shutil.copy2(backup_file, original_file)
                    logger.info(f"Restored from backup: {original_file}")
            except Exception as restore_error:
                logger.error(f"Failed to restore {backup_file}: {restore_error}")

        return 1

    except Exception as e:
        logger.error(f"\n‚ùå Unexpected error: {e}")
        logger.debug("Full traceback:", exc_info=True)
        logger.info("\nSuggestion: Check logs for details")
        return 1

    finally:
        # Resource cleanup
        logger.debug("Cleanup complete")


if __name__ == "__main__":
    import argparse
    import os

    parser = argparse.ArgumentParser(
        description="Export and deduplicate current session (fully automated)"
    )
    parser.add_argument(
        "--description",
        help="Checkpoint description"
    )
    parser.add_argument(
        "--checkpoint-only",
        action="store_true",
        help="Skip deduplication, just create checkpoint"
    )
    parser.add_argument(
        "--auto-compact",
        action="store_true",
        help="Prompt user to /compact after completion"
    )
    parser.add_argument(
        "-y", "--yes",
        action="store_true",
        help="Skip interactive prompts (auto-accept)"
    )
    parser.add_argument(
        "--no-archive",
        action="store_true",
        help="Don't move exports to archive (keep in place)"
    )

    args = parser.parse_args()

    sys.exit(run_export_dedup(
        description=args.description,
        checkpoint_only=args.checkpoint_only,
        auto_compact=args.auto_compact,
        yes=args.yes,
        archive=not args.no_archive
    ))
